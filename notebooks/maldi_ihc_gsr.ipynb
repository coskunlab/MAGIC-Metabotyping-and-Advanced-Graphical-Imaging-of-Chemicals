{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4dba142-6fb6-40eb-85a6-9f952d29b700",
   "metadata": {},
   "source": [
    "### Guided Super-Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc029ae-8bf6-4ad0-a926-851cad862fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "data_dir = r\"..\\data\\high_dose\"\n",
    "#data_dir = r\"..\\data\\low_dose\"\n",
    "#data_dir = r\"..\\data\\no_dose\"\n",
    "\n",
    "fluorescence = np.load(os.path.join(data_dir, \"fluorescence.npy\"))\n",
    "fluorescence_tritc = np.load(os.path.join(data_dir, \"fluorescence_tritc.npy\"))\n",
    "maldi_ihc = np.load(os.path.join(data_dir, \"maldi_ihc.npy\"))\n",
    "lipid = np.load(os.path.join(data_dir, \"lipid.npy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac23dcaf-552e-4a26-91de-4fc21097cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pytorch_msssim import ssim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "# A small helper block: Conv -> BN -> ReLU -> Conv -> BN -> ReLU\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(DoubleConv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# Downsampling block: MaxPool -> DoubleConv\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Down, self).__init__()\n",
    "        self.maxpool = nn.MaxPool2d(2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Upsampling block\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super(Up, self).__init__()\n",
    "        if bilinear:\n",
    "            # Use bilinear upsampling\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            # Then reduce channels via DoubleConv\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "        else:\n",
    "            # Use a transposed conv\n",
    "            self.up = nn.ConvTranspose2d(in_channels // 2, in_channels // 2,\n",
    "                                         kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        \"\"\"\n",
    "        x1 = feature map from the previous decoder layer\n",
    "        x2 = skip connection from the encoder\n",
    "        \"\"\"\n",
    "        # Upsample x1\n",
    "        x1 = self.up(x1)\n",
    "        \n",
    "        # Adjust padding if needed (for odd dimension shapes)\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "\n",
    "        # Concatenate skip connection\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# Final 1x1 output conv\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "# -------------------------------------------------------\n",
    "# Two-Input U-Net\n",
    "# -------------------------------------------------------\n",
    "class TwoInputUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A U-Net with two separate encoders for src and ref images,\n",
    "    then merges features for the decoder.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_channels=1, n_classes=1, bilinear=True):\n",
    "        super(TwoInputUNet, self).__init__()\n",
    "        \n",
    "        # ---------- Encoder for src ----------\n",
    "        self.src_inc   = DoubleConv(n_channels, 64)\n",
    "        self.src_down1 = Down(64, 128)\n",
    "        self.src_down2 = Down(128, 256)\n",
    "        self.src_down3 = Down(256, 512)\n",
    "        self.src_down4 = Down(512, 512)  # bottom layer\n",
    "\n",
    "        # ---------- Encoder for ref ----------\n",
    "        self.ref_inc   = DoubleConv(n_channels, 64)\n",
    "        self.ref_down1 = Down(64, 128)\n",
    "        self.ref_down2 = Down(128, 256)\n",
    "        self.ref_down3 = Down(256, 512)\n",
    "        self.ref_down4 = Down(512, 512)  # bottom layer\n",
    "\n",
    "        # ---------- Decoder ----------\n",
    "        # Notice in_channels for Up is doubled because we concatenate from src + ref encoders\n",
    "        self.up1  = Up(512*2, 256, bilinear)\n",
    "        self.up2  = Up(256*2, 128, bilinear)\n",
    "        self.up3  = Up(128*2, 64,  bilinear)\n",
    "        self.up4  = Up(64*2,  64,  bilinear)\n",
    "        self.outc = OutConv(64, n_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, src, ref):\n",
    "        # 1) Encode src\n",
    "        src_x1 = self.src_inc(src)        \n",
    "        src_x2 = self.src_down1(src_x1)   \n",
    "        src_x3 = self.src_down2(src_x2)   \n",
    "        src_x4 = self.src_down3(src_x3)   \n",
    "        src_x5 = self.src_down4(src_x4)   \n",
    "    \n",
    "        # 2) Encode ref\n",
    "        ref_x1 = self.ref_inc(ref)        \n",
    "        ref_x2 = self.ref_down1(ref_x1)   \n",
    "        ref_x3 = self.ref_down2(ref_x2)   \n",
    "        ref_x4 = self.ref_down3(ref_x3)   \n",
    "        ref_x5 = self.ref_down4(ref_x4)   \n",
    "    \n",
    "        # Instead of cat, do sums:\n",
    "        bottom = src_x5 + ref_x5       # shape [B, 512, ...]\n",
    "        skip4  = src_x4 + ref_x4       # shape [B, 512, ...]\n",
    "        skip3  = src_x3 + ref_x3       # shape [B, 256, ...]\n",
    "        skip2  = src_x2 + ref_x2       # shape [B, 128, ...]\n",
    "        skip1  = src_x1 + ref_x1       # shape [B, 64,  ...]\n",
    "        \n",
    "        # Now decode using a standard single‐U‐Net logic\n",
    "        x = self.up1(bottom, skip4)  \n",
    "        x = self.up2(x, skip3)\n",
    "        x = self.up3(x, skip2)\n",
    "        x = self.up4(x, skip1)\n",
    "        x = self.outc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64212736-c0e7-47bb-be41-a4993e05db3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "from super_image import EdsrModel, ImageLoader\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.exposure import match_histograms\n",
    "\n",
    "# Function to split an image into non-overlapping tiles\n",
    "def split_into_tiles(image, tile_size=500):\n",
    "    tiles = []\n",
    "    for i in range(0, image.shape[0], tile_size):\n",
    "        for j in range(0, image.shape[1], tile_size):\n",
    "            tile = image[i:i+tile_size, j:j+tile_size]  # Extract tile\n",
    "            if tile.shape[0] == tile_size and tile.shape[1] == tile_size:  # Ensure full size\n",
    "                tiles.append(tile)\n",
    "    return np.array(tiles)\n",
    "\n",
    "def match_histogram(pred_np, target_np):\n",
    "    \"\"\"\n",
    "    pred_np   : np.ndarray (H, W)  – network output in [0,1]\n",
    "    target_np : np.ndarray (H, W)  – MALDI tile in [0,1]\n",
    "    \"\"\"\n",
    "    return match_histograms(pred_np, target_np, channel_axis=None)\n",
    "\n",
    "# Function to reconstruct the image from tiles\n",
    "def reconstruct_from_tiles(tiles, original_size=(10000, 10000), tile_size=500):\n",
    "    reconstructed = np.zeros(original_size, dtype=np.float32)\n",
    "    index = 0\n",
    "    for i in range(0, original_size[0], tile_size):\n",
    "        for j in range(0, original_size[1], tile_size):\n",
    "            reconstructed[i:i+tile_size, j:j+tile_size] = tiles[index]\n",
    "            index += 1\n",
    "    return reconstructed\n",
    "\n",
    "def edsr(cropped_maldi):\n",
    "    # Load the EDSR model (scale 2, 3, and 4 models available)\n",
    "    model = EdsrModel.from_pretrained('eugenesiow/edsr-base', scale=4)\n",
    "    \n",
    "    # Ensure proper normalization before conversion to uint8\n",
    "    cropped_maldi_edsr = cropped_maldi.copy()\n",
    "    \n",
    "    # Normalize to [0,1] range\n",
    "    min_val, max_val = cropped_maldi_edsr.min(), cropped_maldi_edsr.max()\n",
    "    if max_val > min_val:  # Avoid division by zero\n",
    "        cropped_maldi_edsr = (cropped_maldi_edsr - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        cropped_maldi_edsr = np.zeros_like(cropped_maldi_edsr)  # If all values are the same, set to zero\n",
    "    \n",
    "    # Convert to uint8 (0-255 range)\n",
    "    cropped_maldi_edsr = (cropped_maldi_edsr * 255).astype(np.uint8)\n",
    "    \n",
    "    # Convert to PIL image and ensure it's RGB for model input\n",
    "    low_res_img_pil = Image.fromarray(cropped_maldi_edsr).convert(\"RGB\")\n",
    "    \n",
    "    # Process the image using EDSR\n",
    "    inputs = ImageLoader.load_image(low_res_img_pil)\n",
    "    preds = model(inputs)\n",
    "    \n",
    "    # Convert tensor to NumPy array\n",
    "    sr_image_np = preds.cpu().detach().numpy().squeeze()  # Remove batch dimension if present\n",
    "    sr_image_np = (sr_image_np * 255).clip(0, 255).astype(np.uint8)  # Scale to 0-255 and convert to uint8\n",
    "    \n",
    "    # Convert to PIL Image (ensure correct channel order)\n",
    "    if sr_image_np.ndim == 3:  # Check if the result is multi-channel (RGB)\n",
    "        sr_image_pil = Image.fromarray(np.transpose(sr_image_np, (1, 2, 0)))  # Convert CHW to HWC\n",
    "    else:\n",
    "        sr_image_pil = Image.fromarray(sr_image_np)\n",
    "    \n",
    "    # Convert to grayscale with proper normalization\n",
    "    sr_image_np_gray = np.array(sr_image_pil.convert(\"L\"), dtype=np.float32)\n",
    "    \n",
    "    min_gray, max_gray = sr_image_np_gray.min(), sr_image_np_gray.max()\n",
    "    if max_gray > min_gray:  \n",
    "        sr_image_np_gray = (sr_image_np_gray - min_gray) / (max_gray - min_gray)  # Normalize grayscale values\n",
    "    else:\n",
    "        sr_image_np_gray = np.zeros_like(sr_image_np_gray)  # If all values are the same, set to zero\n",
    "    \n",
    "    sr_image_np_gray = (sr_image_np_gray * 255).astype(np.uint8)  # Convert to uint8\n",
    "    \n",
    "    cropped_maldi_edsr = sr_image_np_gray\n",
    "    cropped_maldi_edsr = (cropped_maldi_edsr/255.0).astype('float32')\n",
    "    return cropped_maldi_edsr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ad26f6-4a1f-4935-8d3f-931489e372f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "results_dir = r\"..\\results\\high_dose\"\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "ssim_weight = 0.15\n",
    "\n",
    "for channel in range(23):\n",
    "    print(f'Processing: Channel {channel}')   \n",
    "        \n",
    "    # Split into tiles\n",
    "    fluorescence_tiles = split_into_tiles(fluorescence, 1024)\n",
    "    fluorescence_tiles = (fluorescence_tiles/255.0).astype('float32')\n",
    "\n",
    "    fluorescence_tritc_tiles = split_into_tiles(fluorescence_tritc, 1024)\n",
    "    fluorescence_tritc_tiles = (fluorescence_tritc_tiles/255.0).astype('float32')\n",
    "    \n",
    "    maldi_ihc_channel = maldi_ihc[channel,:,:].copy()\n",
    "    maldi_tiles_orig = split_into_tiles(maldi_ihc_channel, 128)\n",
    "    maldi_tiles_orig = (maldi_tiles_orig/255.0).astype('float32')\n",
    "\n",
    "    maldi_tiles = maldi_tiles_orig.copy()\n",
    " \n",
    "    # Base directory for saving tiles\n",
    "    base_dir = os.path.join(results_dir, f\"{channel}\")\n",
    "    \n",
    "    # Subdirectories\n",
    "    output_dir = os.path.join(base_dir, \"outputs\")  # Processed output tiles\n",
    "    fluorescence_dir = os.path.join(base_dir, \"fluorescence\")  # Fluorescence input tiles\n",
    "    fluorescence_tritc_dir = os.path.join(base_dir, \"fluorescence_tritc\")  # Fluorescence cell tiles\n",
    "    maldi_dir = os.path.join(base_dir, \"maldi\")  # MALDI input tiles\n",
    "    original_maldi_dir = os.path.join(base_dir, \"original_maldi\")  # MALDI input tiles\n",
    "    \n",
    "    # Create directories if they don’t exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    os.makedirs(fluorescence_dir, exist_ok=True)\n",
    "    os.makedirs(fluorescence_tritc_dir, exist_ok=True)\n",
    "    os.makedirs(maldi_dir, exist_ok=True)\n",
    "    os.makedirs(original_maldi_dir, exist_ok=True)\n",
    "    \n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Initialize the model, optimizer, and loss function\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = TwoInputUNet().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "    def combined_loss(pred, src, upsampled):  \n",
    "        mse_loss = nn.MSELoss()(pred, upsampled)\n",
    "        ssim_loss = 1 - ssim(pred, src, data_range=1, size_average=True)\n",
    "        return (1-ssim_weight) * mse_loss + ssim_weight * ssim_loss\n",
    "    \n",
    "    processed_tiles = []\n",
    "    \n",
    "    # Process and save each tile\n",
    "    for i in range(fluorescence_tiles.shape[0]):\n",
    "        print(f\"Processing tile {i}...\")\n",
    "    \n",
    "        model = TwoInputUNet().to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    \n",
    "        fluorescence_tile = fluorescence_tiles[i]\n",
    "        fluorescence_tritc_tile = fluorescence_tritc_tiles[i]\n",
    "        maldi_tile = maldi_tiles[i]\n",
    "        maldi_tile_orig = maldi_tiles_orig[i]\n",
    "        maldi_tile_orig = cv2.resize(maldi_tile_orig,(1024,1024),interpolation=cv2.INTER_AREA)\n",
    "        maldi_tile = cv2.resize(edsr((maldi_tile*255.0).astype('uint8')),(1024,1024),interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Convert to PyTorch tensor and float32\n",
    "        fluorescence_tile = torch.from_numpy(fluorescence_tile)\n",
    "        maldi_tile = torch.from_numpy(maldi_tile)\n",
    "    \n",
    "        # Save input fluorescence and MALDI tiles as images\n",
    "        fluorescence_filename = os.path.join(fluorescence_dir, f\"fluorescence_tile_{i}.png\")\n",
    "        fluorescence_cell_filename = os.path.join(fluorescence_tritc_dir, f\"fluorescence_tile_{i}.png\")\n",
    "        maldi_filename = os.path.join(maldi_dir, f\"maldi_tile_{i}.png\")\n",
    "        maldi_filename_orig = os.path.join(original_maldi_dir, f\"maldi_tile_{i}.png\")\n",
    "    \n",
    "        plt.imsave(fluorescence_filename, fluorescence_tile, cmap='gray')  # Save fluorescence\n",
    "        plt.imsave(fluorescence_cell_filename, fluorescence_cell_tile, cmap='gray')  # Save fluorescence cell\n",
    "        plt.imsave(maldi_filename, maldi_tile)  # Save MALDI\n",
    "        plt.imsave(maldi_filename_orig, maldi_tile_orig)  # Save MALDI\n",
    "    \n",
    "        # Ensure proper shape (B, C, H, W)\n",
    "        fluorescence_tile = fluorescence_tile.to(device).unsqueeze(0).unsqueeze(0)\n",
    "        maldi_tile = maldi_tile.to(device).unsqueeze(0).unsqueeze(0)\n",
    "    \n",
    "        num_epochs = 100\n",
    "        for epoch in range(num_epochs):\n",
    "            model.train()\n",
    "            epoch_loss = 0\n",
    "    \n",
    "            outputs = model(fluorescence_tile, maldi_tile)   # <--- TWO inputs\n",
    "            loss = combined_loss(outputs, fluorescence_tile, maldi_tile)\n",
    "    \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "    \n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "        # Convert output to NumPy array\n",
    "        processed_tile = torch.sigmoid(outputs).cpu().squeeze().detach().numpy()\n",
    "        processed_tile = match_histogram(processed_tile, maldi_tile_orig)\n",
    "        processed_tiles.append(processed_tile)\n",
    "    \n",
    "        # Save the processed output tile as an image\n",
    "        output_filename = os.path.join(output_dir, f\"output_tile_{i}.png\")\n",
    "        plt.imsave(output_filename, processed_tile)\n",
    "    \n",
    "        print(f\"Saved processed tile {i} to {output_filename}\")\n",
    "    \n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a4eae4-a81c-4a4c-acf5-a8854976475f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "def stitch_tiles_blended(tiles, grid_shape=(5, 5), tile_size=1000, overlap=100, blend=True):\n",
    "    step = tile_size - overlap\n",
    "    out_size = step * grid_shape[0] + overlap\n",
    "    stitched = np.zeros((out_size, out_size), dtype=np.float32)\n",
    "    weight_map = np.zeros_like(stitched)\n",
    "\n",
    "    if blend and overlap > 0:\n",
    "        def blend_window(size, overlap):\n",
    "            w = np.ones(size)\n",
    "            ramp = np.linspace(0, 1, overlap)\n",
    "            w[:overlap] = ramp\n",
    "            w[-overlap:] = ramp[::-1]\n",
    "            return w\n",
    "\n",
    "        blend_x = blend_window(tile_size, overlap)\n",
    "        blend_y = blend_window(tile_size, overlap)\n",
    "        blend_mask = np.outer(blend_y, blend_x)\n",
    "    else:\n",
    "        blend_mask = np.ones((tile_size, tile_size), dtype=np.float32)\n",
    "\n",
    "    for idx, tile in enumerate(tiles):\n",
    "        row = idx // grid_shape[1]\n",
    "        col = idx % grid_shape[1]\n",
    "        y_start = row * step\n",
    "        x_start = col * step\n",
    "\n",
    "        stitched[y_start:y_start+tile_size, x_start:x_start+tile_size] += tile * blend_mask\n",
    "        weight_map[y_start:y_start+tile_size, x_start:x_start+tile_size] += blend_mask\n",
    "\n",
    "    stitched /= np.maximum(weight_map, 1e-6)\n",
    "    return stitched\n",
    "\n",
    "def load_tiles(tile_dir, prefix, grid_shape, tile_size=1000):\n",
    "    total_tiles = grid_shape[0] * grid_shape[1]\n",
    "    tiles = []\n",
    "    for i in range(total_tiles):\n",
    "        tile_path = os.path.join(tile_dir, f\"{prefix}_tile_{i}.png\")\n",
    "        if os.path.exists(tile_path):\n",
    "            img = cv2.imread(tile_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img = img.astype(np.float32) / 255.0\n",
    "        else:\n",
    "            img = np.zeros((tile_size, tile_size), dtype=np.float32)\n",
    "        tiles.append(img)\n",
    "    return tiles\n",
    "\n",
    "# === Main Processing ===\n",
    "for channel in range(23):\n",
    "    print(f\"Stitching Channel {channel}...\")\n",
    "\n",
    "    base_dir = os.path.join(results_dir, f\"{channel}\")\n",
    "    output_dir = os.path.join(base_dir, \"outputs\")\n",
    "    fluorescence_dir = os.path.join(base_dir, \"fluorescence\")\n",
    "    fluorescence_tritc_dir = os.path.join(base_dir, \"fluorescence_tritc\")\n",
    "    maldi_dir = os.path.join(base_dir, \"maldi\")\n",
    "    original_maldi_dir = os.path.join(base_dir, \"original_maldi\")\n",
    "\n",
    "    grid_shape = (5, 5)\n",
    "    tile_size = 1000\n",
    "\n",
    "    # --- Processed Tiles ---\n",
    "    processed_tiles = load_tiles(output_dir, \"output\", grid_shape, tile_size)\n",
    "    stitched_processed = stitch_tiles_blended(processed_tiles, grid_shape, tile_size)\n",
    "    plt.imsave(os.path.join(output_dir, \"stitched.png\"), stitched_processed, cmap='viridis')\n",
    "\n",
    "    # --- Fluorescence Tiles ---\n",
    "    fluorescence_tiles = load_tiles(fluorescence_dir, \"fluorescence\", grid_shape, tile_size)\n",
    "    stitched_fluorescence = stitch_tiles_blended(fluorescence_tiles, grid_shape, tile_size)\n",
    "    plt.imsave(os.path.join(fluorescence_dir, \"stitched.png\"), stitched_fluorescence, cmap='gray')\n",
    "\n",
    "    # --- Fluorescence Cell Tiles ---\n",
    "    fluorescence_cell_tiles = load_tiles(fluorescence_tritc_dir, \"fluorescence\", grid_shape, tile_size)\n",
    "    stitched_fluorescence_cell = stitch_tiles_blended(fluorescence_cell_tiles, grid_shape, tile_size)\n",
    "    plt.imsave(os.path.join(fluorescence_tritc_dir, \"stitched.png\"), stitched_fluorescence_cell, cmap='gray')\n",
    "\n",
    "    # --- MALDI Tiles ---\n",
    "    maldi_tiles = load_tiles(maldi_dir, \"maldi\", grid_shape, tile_size)\n",
    "    stitched_maldi = stitch_tiles_blended(maldi_tiles, grid_shape, tile_size)\n",
    "    plt.imsave(os.path.join(maldi_dir, \"stitched.png\"), stitched_maldi, cmap='viridis')\n",
    "\n",
    "    # --- Original MALDI Tiles ---\n",
    "    orig_maldi_tiles = load_tiles(original_maldi_dir, \"maldi\", grid_shape, tile_size)\n",
    "    stitched_orig_maldi = stitch_tiles_blended(orig_maldi_tiles, grid_shape, tile_size)\n",
    "    plt.imsave(os.path.join(original_maldi_dir, \"stitched.png\"), stitched_orig_maldi, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d821d07-bc63-44b3-ab4e-0aea2e749eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# List of channels used\n",
    "channels = range(23)\n",
    "\n",
    "# Initialize list to collect stitched arrays\n",
    "stitched_arrays = []\n",
    "\n",
    "for channel in channels:\n",
    "    stitched_path = os.path.join(results_dir, str(channel), \"outputs\", \"stitched.png\")\n",
    "    \n",
    "    if os.path.exists(stitched_path):\n",
    "        img = cv2.imread(stitched_path, cv2.IMREAD_GRAYSCALE)\n",
    "        stitched_arrays.append(img.astype(np.float32) / 255.0)\n",
    "    else:\n",
    "        print(f\"Missing stitched image for channel {channel}\")\n",
    "\n",
    "# Stack into a (C, H, W) array\n",
    "stitched_stack = np.stack(stitched_arrays, axis=0)\n",
    "print(\"Final stacked shape:\", stitched_stack.shape)  # (C, H, W)\n",
    "np.save(os.path.join(results_dir, \"maldi_ihc_gsr.npy\"), stitched_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3842abea-14e0-44b2-901c-cf3868dacd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def stitch_tiles_blended(tiles, grid_shape=(5, 5), tile_size=1000, overlap=100, blend=True):\n",
    "    step = tile_size - overlap\n",
    "    out_size = step * grid_shape[0] + overlap\n",
    "    stitched = np.zeros((out_size, out_size), dtype=np.float32)\n",
    "    weight_map = np.zeros_like(stitched)\n",
    "\n",
    "    if blend and overlap > 0:\n",
    "        def blend_window(size, overlap):\n",
    "            w = np.ones(size)\n",
    "            ramp = np.linspace(0, 1, overlap)\n",
    "            w[:overlap] = ramp\n",
    "            w[-overlap:] = ramp[::-1]\n",
    "            return w\n",
    "\n",
    "        blend_x = blend_window(tile_size, overlap)\n",
    "        blend_y = blend_window(tile_size, overlap)\n",
    "        blend_mask = np.outer(blend_y, blend_x)\n",
    "    else:\n",
    "        blend_mask = np.ones((tile_size, tile_size), dtype=np.float32)\n",
    "\n",
    "    for idx, tile in enumerate(tiles):\n",
    "        row = idx // grid_shape[1]\n",
    "        col = idx % grid_shape[1]\n",
    "        y_start = row * step\n",
    "        x_start = col * step\n",
    "\n",
    "        stitched[y_start:y_start+tile_size, x_start:x_start+tile_size] += tile * blend_mask\n",
    "        weight_map[y_start:y_start+tile_size, x_start:x_start+tile_size] += blend_mask\n",
    "\n",
    "    stitched /= np.maximum(weight_map, 1e-6)\n",
    "    return stitched\n",
    "\n",
    "def split_into_tiles(image, tile_size=500):\n",
    "    tiles = []\n",
    "    for i in range(0, image.shape[0], tile_size):\n",
    "        for j in range(0, image.shape[1], tile_size):\n",
    "            tile = image[i:i+tile_size, j:j+tile_size]  # Extract tile\n",
    "            if tile.shape[0] == tile_size and tile.shape[1] == tile_size:  # Ensure full size\n",
    "                tiles.append(tile)\n",
    "    return np.array(tiles)\n",
    "\n",
    "lipids_stitched = []\n",
    "for channel in tqdm(range(lipids_square.shape[0])):\n",
    "    grid_shape = (5, 5)\n",
    "    tile_size = 1000\n",
    "    lipid_square = lipids_square[channel,:,:]\n",
    "    lipid_tiles = split_into_tiles(lipid_square, 1000)\n",
    "    lipid_tiles = (lipid_tiles/255.0).astype('float32')\n",
    "    stitched_lipid = stitch_tiles_blended(lipid_tiles, grid_shape, tile_size)\n",
    "    lipids_stitched.append(stitched_lipid)\n",
    "lipids_stitched = np.stack(lipids_stitched,axis=0)\n",
    "np.save(os.path.join(results_dir,'lipids_stitched.npy'), lipids_stitched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefffc44-28ea-4878-ae33-92187bc984d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "magic",
   "language": "python",
   "name": "magic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
